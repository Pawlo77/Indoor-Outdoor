{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3817ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from transformers import (\n",
    "    SiglipForImageClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoImageProcessor,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "from src.transformers import train_transforms, val_transforms, test_transforms\n",
    "from src.callbacks import CHECKPOINT_DIR\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f8f765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SiglipForImageClassification were not initialized from the model checkpoint at google/siglip2-base-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "SiglipForImageClassification                            [1, 2]                    --\n",
       "├─SiglipVisionTransformer: 1-1                          [1, 768]                  --\n",
       "│    └─SiglipVisionEmbeddings: 2-1                      [1, 196, 768]             --\n",
       "│    │    └─Conv2d: 3-1                                 [1, 768, 14, 14]          590,592\n",
       "│    │    └─Embedding: 3-2                              [1, 196, 768]             150,528\n",
       "│    └─SiglipEncoder: 2-2                               [1, 196, 768]             --\n",
       "│    │    └─ModuleList: 3-3                             --                        85,054,464\n",
       "│    └─LayerNorm: 2-3                                   [1, 196, 768]             1,536\n",
       "│    └─SiglipMultiheadAttentionPoolingHead: 2-4         [1, 768]                  768\n",
       "│    │    └─MultiheadAttention: 3-4                     [1, 1, 768]               2,362,368\n",
       "│    │    └─LayerNorm: 3-5                              [1, 1, 768]               1,536\n",
       "│    │    └─SiglipMLP: 3-6                              [1, 1, 768]               4,722,432\n",
       "├─Linear: 1-2                                           [1, 2]                    1,538\n",
       "=========================================================================================================\n",
       "Total params: 92,885,762\n",
       "Trainable params: 92,885,762\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 205.69\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 162.61\n",
       "Params size (MB): 362.09\n",
       "Estimated Total Size (MB): 525.30\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"google/siglip2-base-patch16-224\"\n",
    "\n",
    "model = SiglipForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2,\n",
    ")\n",
    "processor = AutoImageProcessor.from_pretrained(checkpoint, use_fast=True)\n",
    "\n",
    "summary(model, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1b0679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(\n",
    "        pred.predictions[0]\n",
    "        if isinstance(pred.predictions, tuple)\n",
    "        else pred.predictions,\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average=\"weighted\")\n",
    "    recall = recall_score(labels, preds, average=\"weighted\")\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    return {\"pixel_values\": torch.stack(images), \"labels\": torch.tensor(labels)}\n",
    "\n",
    "\n",
    "def evaluate_model(trainer, limit=None):\n",
    "    test_ds = ImageFolder(\n",
    "        os.path.join(\"datasets\", \"rest_test\"), transform=test_transforms\n",
    "    )\n",
    "    if limit is not None:\n",
    "        test_ds = Subset(test_ds, range(limit))\n",
    "    score_rest = trainer.evaluate(test_ds)\n",
    "\n",
    "    test_ds = ImageFolder(\n",
    "        os.path.join(\"datasets\", \"wit_test\"), transform=test_transforms\n",
    "    )\n",
    "    if limit is not None:\n",
    "        test_ds = Subset(test_ds, range(limit))\n",
    "    score_wit = trainer.evaluate(test_ds)\n",
    "\n",
    "    return pd.DataFrame([score_rest, score_wit], index=[\"rest\", \"wit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1588c53",
   "metadata": {},
   "source": [
    "# Other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30784218",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DIR = os.path.join(CHECKPOINT_DIR, \"siglip2\", \"other\")\n",
    "os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "\n",
    "train_ds = ImageFolder(\n",
    "    os.path.join(\"datasets\", \"rest_train\"), transform=train_transforms\n",
    ")\n",
    "# train_ds = Subset(train_ds, range(100))\n",
    "\n",
    "val_ds = ImageFolder(os.path.join(\"datasets\", \"rest_val\"), transform=val_transforms)\n",
    "# val_ds = Subset(val_ds, range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "768077ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3450' max='575000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3450/575000 13:40:19 < 2266:19:48, 0.07 it/s, Epoch 6/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.359900</td>\n",
       "      <td>0.292432</td>\n",
       "      <td>0.885503</td>\n",
       "      <td>0.891092</td>\n",
       "      <td>0.885503</td>\n",
       "      <td>0.885800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>0.388499</td>\n",
       "      <td>0.823901</td>\n",
       "      <td>0.851418</td>\n",
       "      <td>0.823901</td>\n",
       "      <td>0.816936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.359900</td>\n",
       "      <td>0.366436</td>\n",
       "      <td>0.833914</td>\n",
       "      <td>0.837699</td>\n",
       "      <td>0.833914</td>\n",
       "      <td>0.832022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.381300</td>\n",
       "      <td>0.406820</td>\n",
       "      <td>0.815629</td>\n",
       "      <td>0.822322</td>\n",
       "      <td>0.815629</td>\n",
       "      <td>0.816095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>0.404702</td>\n",
       "      <td>0.819983</td>\n",
       "      <td>0.823476</td>\n",
       "      <td>0.819983</td>\n",
       "      <td>0.820429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.403304</td>\n",
       "      <td>0.810623</td>\n",
       "      <td>0.814234</td>\n",
       "      <td>0.810623</td>\n",
       "      <td>0.808270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=TARGET_DIR,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1000,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_dir=os.path.join(TARGET_DIR, \"logs\"),\n",
    "    logging_steps=100,\n",
    "    logging_first_step=True,\n",
    "    warmup_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    # metric_for_best_model=\"f1\",\n",
    "    # greater_is_better=True,\n",
    "    save_total_limit=3,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Saving the model...\")\n",
    "finally:\n",
    "    model.save_pretrained(os.path.join(TARGET_DIR, \"model\"))\n",
    "    processor.save_pretrained(os.path.join(TARGET_DIR, \"processor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ea6b8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='457' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [144/144 10:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rest</th>\n",
       "      <th>wit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>0.289871</td>\n",
       "      <td>0.312350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.885963</td>\n",
       "      <td>0.878700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_precision</th>\n",
       "      <td>0.892775</td>\n",
       "      <td>0.879517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_recall</th>\n",
       "      <td>0.885963</td>\n",
       "      <td>0.878700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_f1</th>\n",
       "      <td>0.886357</td>\n",
       "      <td>0.874884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_runtime</th>\n",
       "      <td>459.826400</td>\n",
       "      <td>171.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <td>9.993000</td>\n",
       "      <td>58.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <td>0.313000</td>\n",
       "      <td>1.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               rest         wit\n",
       "eval_loss                  0.289871    0.312350\n",
       "eval_accuracy              0.885963    0.878700\n",
       "eval_precision             0.892775    0.879517\n",
       "eval_recall                0.885963    0.878700\n",
       "eval_f1                    0.886357    0.874884\n",
       "eval_runtime             459.826400  171.786300\n",
       "eval_samples_per_second    9.993000   58.212000\n",
       "eval_steps_per_second      0.313000    1.822000\n",
       "epoch                      6.000000    6.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(trainer=trainer).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74088ff3",
   "metadata": {},
   "source": [
    "# Our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46923fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DIR = os.path.join(CHECKPOINT_DIR, \"siglip2\", \"wit\")\n",
    "os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "\n",
    "train_ds = ImageFolder(\n",
    "    os.path.join(\"datasets\", \"wit_train\"), transform=train_transforms\n",
    ")\n",
    "# train_ds = Subset(train_ds, range(100))\n",
    "val_ds = ImageFolder(os.path.join(\"datasets\", \"wit_val\"), transform=val_transforms)\n",
    "# val_ds = Subset(val_ds, range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53fe5271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='1250000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   5000/1250000 8:04:53 < 2013:07:27, 0.17 it/s, Epoch 4/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.204500</td>\n",
       "      <td>0.195957</td>\n",
       "      <td>0.918800</td>\n",
       "      <td>0.921943</td>\n",
       "      <td>0.918800</td>\n",
       "      <td>0.919573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.232362</td>\n",
       "      <td>0.904500</td>\n",
       "      <td>0.906685</td>\n",
       "      <td>0.904500</td>\n",
       "      <td>0.905179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.281600</td>\n",
       "      <td>0.321944</td>\n",
       "      <td>0.870700</td>\n",
       "      <td>0.875127</td>\n",
       "      <td>0.870700</td>\n",
       "      <td>0.865230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.255600</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.889506</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.886241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=TARGET_DIR,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1000,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_dir=os.path.join(TARGET_DIR, \"logs\"),\n",
    "    logging_steps=100,\n",
    "    warmup_steps=500,\n",
    "    logging_first_step=True,\n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    # metric_for_best_model=\"f1\",\n",
    "    # greater_is_better=True,\n",
    "    save_total_limit=3,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Saving the model...\")\n",
    "finally:\n",
    "    model.save_pretrained(os.path.join(TARGET_DIR, \"model\"))\n",
    "    processor.save_pretrained(os.path.join(TARGET_DIR, \"processor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e237a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='457' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [144/144 04:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rest</th>\n",
       "      <th>wit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>0.483702</td>\n",
       "      <td>0.188542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.835909</td>\n",
       "      <td>0.924300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_precision</th>\n",
       "      <td>0.840014</td>\n",
       "      <td>0.927249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_recall</th>\n",
       "      <td>0.835909</td>\n",
       "      <td>0.924300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_f1</th>\n",
       "      <td>0.833695</td>\n",
       "      <td>0.925043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_runtime</th>\n",
       "      <td>80.385900</td>\n",
       "      <td>171.123400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <td>57.162000</td>\n",
       "      <td>58.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <td>1.791000</td>\n",
       "      <td>1.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              rest         wit\n",
       "eval_loss                 0.483702    0.188542\n",
       "eval_accuracy             0.835909    0.924300\n",
       "eval_precision            0.840014    0.927249\n",
       "eval_recall               0.835909    0.924300\n",
       "eval_f1                   0.833695    0.925043\n",
       "eval_runtime             80.385900  171.123400\n",
       "eval_samples_per_second  57.162000   58.437000\n",
       "eval_steps_per_second     1.791000    1.829000\n",
       "epoch                     4.000000    4.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(trainer=trainer).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
